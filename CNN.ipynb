{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJgGc2ezx8Xw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# the following architecture has three linear layers --> \n",
        "# --> overfits the training data\n",
        "\n",
        "class Architecture1(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_classes):\n",
        "    super(Architecture1,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size,num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(hidden_size,num_classes)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(x)\n",
        "    out = self.relu(x)\n",
        "    out = self.fc3(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "# 2 linear layers only\n",
        "class Architecture2(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_classes):\n",
        "    super(Achitecture2,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size,num_classes)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Techniques\n",
        "model = Architecture1(10,20,2)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,weight_decay = 1e-5)\n",
        "\n",
        "#-- Dropout layer in pytorch for regularization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YVEO8SIt08Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking care of the MNIST dataset\n",
        "\n",
        "transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0?1307,),(0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MNIST"
      ],
      "metadata": {
        "id": "pCXo8-K63lU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building CNN model\n",
        "# Arch = Combination of conv2d,MaxPool2d , ReLU , View , LinearLayer\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320,50)\n",
        "    self.fc2 = nn.Linear(50,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
        "    x = x.view(-1,320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x,training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "qCoOTZVP4MOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}